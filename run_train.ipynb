{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/jeanpetit/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/jeanpetit/.config/sagemaker/config.yaml\n",
      "Using default bucket sagemaker-us-east-1-866697407305\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/jeanpetit/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name jean to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/jeanpetit/.config/sagemaker/config.yaml\n",
      "sagemaker rôle arn: arn:aws:iam::866697407305:role/service-role/AmazonSageMaker-ExecutionRole-20230928T232931\n",
      "sagemaker session bucket: sagemaker-us-east-1-866697407305\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = session.default_bucket()\n",
    "    print(f\"Using default bucket {sagemaker_session_bucket}\")\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"AmazonSageMaker-ExecutionRole-20230928T232931\")[\"Role\"][\"Arn\"]\n",
    "    \n",
    "    \n",
    "session = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker rôle arn: {role}\") \n",
    "print(f\"sagemaker session bucket: {session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration databricks--databricks-dolly-15k-7427aa6e57c34282\n",
      "Found cached dataset json (/home/jeanpetit/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category'],\n",
       "    num_rows: 15011\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load an prepare dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'text': '### Instructions\\n What are monsoons in India?\\n\\n### Answer\\nMonsoons in India typically refer to the rainy season, which is generally between June and September. It follows the summer season. Monsoons also refer to the seasonal weather pattern that causes rains in India due to the changing direction of winds. The hot summers cause the air over the land to heat up and rise causing low pressure, which the moisture-filled air from the ocean rushes to fill. As this air rises over the mountains, it cools and brings rain to many parts of the Indian subcontinent.</s>'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to format the dataset\n",
    "def format_function(sample):\n",
    "    instruction = f\"### Instructions\\n {sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample['context']) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    \n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Instructions\\n What's Rockhopper Exploration PLC business?\\n\\n### Context\\nRockhopper Exploration PLC is an oil and gas exploration company headquartered in Salisbury, Wiltshire, United Kingdom. It owns offshore exploration and production licences in the North Falkland Basin in the waters north of the Falkland Islands.\\n\\nRockhopper is listed on the Alternative Investment Market of the London Stock Exchange.\\n\\n### Answer\\nRockhopper Exploration PLC is in the oil and gas business, headquartered in Salisbury, Wiltshire, United Kingdom.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_function(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeanpetit/.local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = \"meta-llama/Llama-2-13b-hf\" # shared weight\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2861d6852ce2435cbe176bf6f1ee13ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15011 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '### Instructions\\n Classify each of the following as a French or Italian company: Campari, Dior, Hermes, Ferrari, Prada\\n\\n### Answer\\nCampari: Italian\\nDior: French\\nHermes: French\\nFerrari: Italian\\nPrada: Italian</s>'}\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_function(sample)}{tokenizer.eos_token}\"\n",
    "    \n",
    "    return sample\n",
    "\n",
    "#  apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "print(dataset[randint(0, len(dataset))])\n",
    "\n",
    "#  empty list to save remainder from bacthes  to use in the next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48750b6c8af245cd857d9ee47414fca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754d7ca23f304135a661e41e7ee0ffa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 15011\n"
     ]
    }
   ],
   "source": [
    "def chunck(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in the next batch\n",
    "    \n",
    "    global remainder\n",
    "    \n",
    "    # concatenate all texts and add remainder from previous batch\n",
    "    concatenated_samples = {k: list(chain(*sample[k])) for k  in sample.keys()}\n",
    "    concatenated_samples = {k: remainder[k] + concatenated_samples[k] for k in concatenated_samples.keys()}\n",
    "    \n",
    "    # get total numbers of tokens fro batch\n",
    "    batch_total_length = len(concatenated_samples[list(sample.keys())[0]])\n",
    "    \n",
    "    # get max number chunk of batch\n",
    "    if batch_total_length > chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "        \n",
    "    # split by chunck of max length\n",
    "    result = {\n",
    "        k: [t[i :  i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_samples.items()\n",
    "    }\n",
    "    \n",
    "    \n",
    "# tokenize and chunck dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample['text']), batched=True, remove_columns=list(dataset.features)).map(\n",
    "        partial(chunck, chunk_length=2048),\n",
    "        batched=True,\n",
    ")\n",
    "    \n",
    "    \n",
    "#  print total numbers of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-866697407305/processed/llama/dolly/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{session.default_bucket()}/processed/llama/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/jeanpetit/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2023-10-04-15-29-20-2023-10-04-14-44-24-482\n"
     ]
    },
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g5.4xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/home/jeanpetit/Bureau/Project/AI/demo_sagemake/run_train.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jeanpetit/Bureau/Project/AI/demo_sagemake/run_train.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m: training_input_path}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jeanpetit/Bureau/Project/AI/demo_sagemake/run_train.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# starting the train job with our uploaded datasets as input\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jeanpetit/Bureau/Project/AI/demo_sagemake/run_train.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m huggingface_estimator\u001b[39m.\u001b[39;49mfit(data, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/estimator.py:1311\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_for_training(job_name\u001b[39m=\u001b[39mjob_name)\n\u001b[1;32m   1310\u001b[0m experiment_config \u001b[39m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m-> 1311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job \u001b[39m=\u001b[39m _TrainingJob\u001b[39m.\u001b[39;49mstart_new(\u001b[39mself\u001b[39;49m, inputs, experiment_config)\n\u001b[1;32m   1312\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1313\u001b[0m \u001b[39mif\u001b[39;00m wait:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/estimator.py:2374\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001b[39;00m\n\u001b[1;32m   2350\u001b[0m \n\u001b[1;32m   2351\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[39m    all information about the started training job.\u001b[39;00m\n\u001b[1;32m   2371\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m train_args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_train_args(estimator, inputs, experiment_config)\n\u001b[0;32m-> 2374\u001b[0m estimator\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_args)\n\u001b[1;32m   2376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(estimator\u001b[39m.\u001b[39msagemaker_session, estimator\u001b[39m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/session.py:931\u001b[0m, in \u001b[0;36mSession.train\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, infra_check_config, container_entry_point, container_arguments, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    928\u001b[0m     LOGGER\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mtrain request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, json\u001b[39m.\u001b[39mdumps(request, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_training_job(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest)\n\u001b[0;32m--> 931\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_intercept_create_request(train_request, submit, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/session.py:5608\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   5591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_intercept_create_request\u001b[39m(\n\u001b[1;32m   5592\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   5593\u001b[0m     request: typing\u001b[39m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5596\u001b[0m     \u001b[39m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   5597\u001b[0m ):\n\u001b[1;32m   5598\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   5599\u001b[0m \n\u001b[1;32m   5600\u001b[0m \u001b[39m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5606\u001b[0m \u001b[39m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   5607\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5608\u001b[0m     \u001b[39mreturn\u001b[39;00m create(request)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sagemaker/session.py:929\u001b[0m, in \u001b[0;36mSession.train.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    927\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating training-job with name: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, job_name)\n\u001b[1;32m    928\u001b[0m LOGGER\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mtrain request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, json\u001b[39m.\u001b[39mdumps(request, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[0;32m--> 929\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_client\u001b[39m.\u001b[39;49mcreate_training_job(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g5.4xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota."
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
